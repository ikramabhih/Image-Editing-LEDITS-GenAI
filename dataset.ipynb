{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8e0536a-9df2-4579-8154-bf2922610ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\user\\anaconda3\\lib\\site-packages (4.4.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: pillow in c:\\users\\user\\anaconda3\\lib\\site-packages (11.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (2.1.3)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.3.2)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.11.10)\n",
      "Requirement already satisfied: anyio in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (4.7.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.18.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "pip install datasets pillow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7442bd60-1d95-4827-be5b-4c3ba0eeec29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: pyarrow 22.0.0Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Uninstalling pyarrow-22.0.0:\n",
      "  Successfully uninstalled pyarrow-22.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\user\\anaconda3\\Lib\\site-packages\\~yarrow.libs'.\n",
      "You can safely remove it manually.\n",
      "WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\user\\anaconda3\\Lib\\site-packages\\~yarrow'.\n",
      "You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall pyarrow -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef700838-4016-42a1-adf9-1e73621560d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in c:\\users\\user\\anaconda3\\lib\\site-packages (22.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyarrow --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5da93426-d166-401e-951f-7725a443d0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyArrow Parquet OK !\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "print(\"PyArrow Parquet OK !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4307f752-e5bb-4c6e-9fb8-5dcb5250e5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub[hf_xet] in c:\\users\\user\\anaconda3\\lib\\site-packages (0.36.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub[hf_xet]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (2025.11.12)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install huggingface_hub[hf_xet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63ce5825-ac6f-4ebe-b24e-36ae1e428a3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Téléchargement du dataset (30%) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e622bd19ba4e4e1f971b151935cc17a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/262 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "604ba1c6b2004ea8bb2a7e032c344396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train: 100%|████████████████████████████████████████████████████████| 75122/75122 [2:13:32<00:00,  9.38it/s]\n",
      "Processing val: 100%|██████████████████████████████████████████████████████████████| 9390/9390 [16:05<00:00,  9.73it/s]\n",
      "Processing test: 100%|█████████████████████████████████████████████████████████████| 9391/9391 [16:05<00:00,  9.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tous les splits sont prêts dans : dataset_leditspp_256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Télécharger 30% du dataset\n",
    "print(\"Téléchargement du dataset (30%) ...\")\n",
    "dataset = load_dataset(\"timbrooks/instructpix2pix-clip-filtered\", split=\"train[:30%]\")\n",
    "\n",
    "# 2. Diviser le dataset en train (80%), validation (10%) et test (10%)\n",
    "train_dataset, temp_dataset = dataset.train_test_split(test_size=0.2, seed=42).values()\n",
    "val_dataset, test_dataset = temp_dataset.train_test_split(test_size=0.5, seed=42).values()\n",
    "\n",
    "splits = {\n",
    "    \"train\": train_dataset,\n",
    "    \"val\": val_dataset,\n",
    "    \"test\": test_dataset\n",
    "}\n",
    "\n",
    "# 3. Fonction pour sauvegarder les images et le metadata (résolution 256x256)\n",
    "def save_dataset(split_name, split_dataset, base_dir=\"dataset_leditspp_256\"):\n",
    "    split_dir = os.path.join(base_dir, split_name)\n",
    "    os.makedirs(split_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(split_dir, \"original\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(split_dir, \"edited\"), exist_ok=True)\n",
    "\n",
    "    meta_path = os.path.join(split_dir, \"metadata.tsv\")\n",
    "    with open(meta_path, \"w\", encoding=\"utf-8\") as meta:\n",
    "        meta.write(\"original_image_path\\tedited_image_path\\tedit_prompt\\toriginal_prompt\\tedited_prompt\\n\")\n",
    "\n",
    "        for i, sample in tqdm(enumerate(split_dataset), total=len(split_dataset), desc=f\"Processing {split_name}\"):\n",
    "            try:\n",
    "                # Redimensionner les images à 256x256\n",
    "                orig_img = sample[\"original_image\"].resize((256, 256), Image.LANCZOS)\n",
    "                edit_img = sample[\"edited_image\"].resize((256, 256), Image.LANCZOS)\n",
    "\n",
    "                orig_path = f\"original/{i:06d}.png\"\n",
    "                edit_path = f\"edited/{i:06d}.png\"\n",
    "\n",
    "                orig_img.save(os.path.join(split_dir, orig_path))\n",
    "                edit_img.save(os.path.join(split_dir, edit_path))\n",
    "\n",
    "                edit_prompt = sample[\"edit_prompt\"].replace(\"\\t\", \" \").replace(\"\\n\", \" \")\n",
    "                original_prompt = sample[\"original_prompt\"].replace(\"\\t\", \" \").replace(\"\\n\", \" \")\n",
    "                edited_prompt = sample[\"edited_prompt\"].replace(\"\\t\", \" \").replace(\"\\n\", \" \")\n",
    "\n",
    "                meta.write(f\"{orig_path}\\t{edit_path}\\t{edit_prompt}\\t{original_prompt}\\t{edited_prompt}\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur sur l’échantillon {i}: {e}\")\n",
    "                continue\n",
    "\n",
    "# 4. Sauvegarder chaque split\n",
    "for split_name, split_dataset in splits.items():\n",
    "    save_dataset(split_name, split_dataset)\n",
    "\n",
    "print(\"Tous les splits sont prêts dans :\", \"dataset_leditspp_256\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "464e0e83-b771-4acc-80ed-4935e6fa3c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nettoyage final terminé : 75122 lignes gardées, 0 lignes supprimées.\n",
      "Nouveau fichier corrigé : dataset_leditspp_256/train/metadata_fixed.tsv\n"
     ]
    }
   ],
   "source": [
    "input_path = \"dataset_leditspp_256/train/metadata.tsv\"\n",
    "output_path_fixed = \"dataset_leditspp_256/train/metadata_fixed.tsv\"\n",
    "\n",
    "cleaned = 0\n",
    "removed = 0\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as fin, open(output_path_fixed, \"w\", encoding=\"utf-8\") as fout:\n",
    "    header = fin.readline().strip()\n",
    "    fout.write(header + \"\\n\")\n",
    "\n",
    "    for line in fin:\n",
    "        # Supprimer guillemets isolés et espaces parasites\n",
    "        line = line.replace('\"', '').strip()\n",
    "\n",
    "        # Diviser selon les tabulations\n",
    "        parts = line.split(\"\\t\")\n",
    "\n",
    "        # Si on a plus ou moins de 5 colonnes → ignorer la ligne\n",
    "        if len(parts) == 5:\n",
    "            fout.write(\"\\t\".join(parts) + \"\\n\")\n",
    "            cleaned += 1\n",
    "        else:\n",
    "            removed += 1\n",
    "\n",
    "print(f\"Nettoyage final terminé : {cleaned} lignes gardées, {removed} lignes supprimées.\")\n",
    "print(\"Nouveau fichier corrigé :\", output_path_fixed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d60856a-6fc1-489e-974f-214f4fc8a5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nettoyage final terminé : 9391 lignes gardées, 0 lignes supprimées.\n",
      "Nouveau fichier corrigé : dataset_leditspp_256/test/metadata_fixed.tsv\n"
     ]
    }
   ],
   "source": [
    "input_path = \"dataset_leditspp_256/test/metadata.tsv\"\n",
    "output_path_fixed = \"dataset_leditspp_256/test/metadata_fixed.tsv\"\n",
    "\n",
    "cleaned = 0\n",
    "removed = 0\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as fin, open(output_path_fixed, \"w\", encoding=\"utf-8\") as fout:\n",
    "    header = fin.readline().strip()\n",
    "    fout.write(header + \"\\n\")\n",
    "\n",
    "    for line in fin:\n",
    "        # Supprimer guillemets isolés et espaces parasites\n",
    "        line = line.replace('\"', '').strip()\n",
    "\n",
    "        # Diviser selon les tabulations\n",
    "        parts = line.split(\"\\t\")\n",
    "\n",
    "        # Si on a plus ou moins de 5 colonnes → ignorer la ligne\n",
    "        if len(parts) == 5:\n",
    "            fout.write(\"\\t\".join(parts) + \"\\n\")\n",
    "            cleaned += 1\n",
    "        else:\n",
    "            removed += 1\n",
    "\n",
    "print(f\"Nettoyage final terminé : {cleaned} lignes gardées, {removed} lignes supprimées.\")\n",
    "print(\"Nouveau fichier corrigé :\", output_path_fixed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10da7fe6-e58f-489e-9401-09b4fa08f118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nettoyage final terminé : 9390 lignes gardées, 0 lignes supprimées.\n",
      "Nouveau fichier corrigé : dataset_leditspp_256/val/metadata_fixed.tsv\n"
     ]
    }
   ],
   "source": [
    "input_path = \"dataset_leditspp_256/val/metadata.tsv\"\n",
    "output_path_fixed = \"dataset_leditspp_256/val/metadata_fixed.tsv\"\n",
    "\n",
    "cleaned = 0\n",
    "removed = 0\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as fin, open(output_path_fixed, \"w\", encoding=\"utf-8\") as fout:\n",
    "    header = fin.readline().strip()\n",
    "    fout.write(header + \"\\n\")\n",
    "\n",
    "    for line in fin:\n",
    "        # Supprimer guillemets isolés et espaces parasites\n",
    "        line = line.replace('\"', '').strip()\n",
    "\n",
    "        # Diviser selon les tabulations\n",
    "        parts = line.split(\"\\t\")\n",
    "\n",
    "        # Si on a plus ou moins de 5 colonnes → ignorer la ligne\n",
    "        if len(parts) == 5:\n",
    "            fout.write(\"\\t\".join(parts) + \"\\n\")\n",
    "            cleaned += 1\n",
    "        else:\n",
    "            removed += 1\n",
    "\n",
    "print(f\"Nettoyage final terminé : {cleaned} lignes gardées, {removed} lignes supprimées.\")\n",
    "print(\"Nouveau fichier corrigé :\", output_path_fixed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92e4f1f8-c658-442c-ac94-41e0c60087d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement réussi : 75122 lignes valides\n",
      "   original_image_path  edited_image_path                   edit_prompt  \\\n",
      "0  original/000000.png  edited/000000.png  Make it a landscape painting   \n",
      "1  original/000001.png  edited/000001.png       Add a desert background   \n",
      "2  original/000002.png  edited/000002.png             Add a storm cloud   \n",
      "3  original/000003.png  edited/000003.png      make the sheep a giraffe   \n",
      "4  original/000004.png  edited/000004.png          make it a watercolor   \n",
      "\n",
      "                                     original_prompt  \\\n",
      "0         Milky way over Reessor lake by Alan Dyer ©   \n",
      "1  Juan Bosco Forest Animals - Elizabeth Taylor a...   \n",
      "2  Wooden barn on top of alpine meadow with rugge...   \n",
      "3  Click image for larger version.  Name:sheep-wi...   \n",
      "4  KEVIN BEERS Clark Point oil on board, 18 x 24 ...   \n",
      "\n",
      "                                       edited_prompt  \n",
      "0  Landscape painting of Milky way over Reessor l...  \n",
      "1  Juan Bosco Forest Animals - Elizabeth Taylor a...  \n",
      "2  Wooden barn on top of alpine meadow with rugge...  \n",
      "3  Click image for larger version.  Name:giraffe-...  \n",
      "4  KEVIN BEERS Clark Point watercolor, 18 x 24 in...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(output_path_fixed, sep=\"\\t\", engine=\"python\")\n",
    "print(\"Chargement réussi :\", len(df), \"lignes valides\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce68a04-af64-4a34-a91f-1c2fc59a9567",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch_env)",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
